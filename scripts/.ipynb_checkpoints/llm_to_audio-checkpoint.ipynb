{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc9edba-2517-4e0e-a228-37bf448e96b6",
   "metadata": {},
   "source": [
    "# LLM to Audio (testcase)\n",
    "\n",
    "**Read the request documentation to tune your model for your application**\n",
    "\n",
    "**Request Documentation**: https://platform.openai.com/docs/api-reference/completions/create\n",
    "\n",
    "**OpenAI Documentation**: https://platform.openai.com/docs/quickstart?context=python\n",
    "\n",
    "**Find your keys here**: https://platform.openai.com/api-keys\n",
    "\n",
    "**Keep an eye on your credit usage**: https://platform.openai.com/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0cbf1d",
   "metadata": {},
   "source": [
    "## OpenAI API Details and Hyper-Parameters\n",
    "\n",
    "**Models**: The model you want to use (i.e. \"gpt-4-turbo\", \"gpt-4\", \"gpt-4o\", \"gpt-4o-mini\" or \"gpt-3.5-turbo\")\n",
    "\n",
    "**Message**: The message being sent (the prompt)\n",
    "\n",
    "**Temperature**: Number between 0 and 1, can be adjusted when calling the OpenAI API to control the randomness of the output generated by the language model. It determines how \"creative\" or \"conservative\" the responses are. A temperature of 1.0 means the model generates text with standard randomness, while a lower temperature (closer to 0.0) will make it more deterministic. Suggestion: Set the temperature to a moderate value, around 0.2 to 0.5. This will give you some randomness, mimicking the variability in human input, but still maintain enough accuracy to ensure that the responses are meaningful. **In the case of our work, we held this constant at a value of 0.2 to get slight stochasticity.** \n",
    "\n",
    "**Frequency_penalty**: Number between -2 and 2, where higher numbers penalize new tokens based on their frequency to that point in the response. The higher the number, the lower the probability of repetition. This parameter penalizes repeated tokens in the output, making responses more diverse and creative. Suggestion: Use a low frequency penalty (e.g., 0.0 to 0.2) to reduce redundancy but still allow the model to use relevant tokens multiple times where necessary. **In the case of our work, we held this constant at a value of 0.0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ecab9-9e60-464c-be72-ee03f0d7c0fc",
   "metadata": {},
   "source": [
    "## Set your GPT Key to your ENV\n",
    "\n",
    "Uncomment this line to check your environment variables. That should be set up in .zshrc or .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985abb6-9a95-419f-9ba6-ce2402af8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07efbe9-72b5-420c-9e6f-e3b21484e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951e29c-cfd5-4882-baa3-46bfbaeeaf0c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd863e0c-f84a-45f9-96cb-543d43b0ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "import random\n",
    "import time\n",
    "random.seed(time.time())\n",
    "\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942974b5-ca3f-432c-b0a7-5b3772ae60a9",
   "metadata": {},
   "source": [
    "## Start Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d558e6-fe57-4190-b9c9-94a9144f94a9",
   "metadata": {},
   "source": [
    "### Build your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11893392",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = [\"stuck\", \"accomplished\", \"progressing\"]\n",
    "\n",
    "all_states_descriptions = [\n",
    "    \"the robot has gotten lost or is stuck behind an obstacle.\",                                            # stuck\n",
    "    \"the robot has successfully reached it's goal and completed its task.\",                                 # accomplished\n",
    "    \"the robot is actively working on the task but has neither gotten stuck nor completed the task.\"        # progressing\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275ebc8-4a5c-4e11-9e2c-bf7d867363e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_assistant_prompt = \"You are an expert roboticist and understand how to design communicative expressions for human-robot interaction.\" \n",
    "\n",
    "robot_morphology = \"small mobile rover\"\n",
    "\n",
    "deployment_context = f\"Consider a scenario where you are collaborating with a {robot_morphology} robot to navigate through a maze and find fruit.\"\n",
    "\n",
    "communication_style = f\"The robot uses nonverbal audio cues to communicate its task status back to the user.\"\n",
    "\n",
    "parameter_01 = \"Parameter 01: [Beats Per Minute (BPM), Controls the speed in which the robot plays its audio cues. If set to ‘slow’ the robot’s audio playback is set to 0.5x speed. If set to ‘medium’ the robot’s audio playback is set to 1.0x speed. If set to ‘high’ the robot’s audio playback is set to 2.0x speed, (slow, medium, fast)]\"\n",
    "parameter_02 = \"Parameter 02: [Beats Per Loop (BPL), Controls the frequency of how many times the robot beeps per second. If set to ‘low’ the robot will beep slowly at 1 time per second. If set to ‘medium’ the robot will beep somewhat rapidly at 2 times per second. If set to ‘high’ the robot will beep rapidly 4 times per second.,(low, medium, high)]\"\n",
    "parameter_03 = \"Parameter 03: [Pitch Bend, Controls the inflection of beeps by bending the pitch. If set to ‘downward’ the robot’s beeps will have downward inflections. If set to ‘neutral’ the robot’s beeps will remain at a neutral unchanged pitch. If set to ‘upward’ the robot’s beeps will have upward inflections.,(downward, neutral, upward)]\"\n",
    "\n",
    "# This is done to randomize the order in which the parameters are presented to GPT, to eliminate any bias for params presented first\n",
    "param_list = [parameter_01, parameter_02, parameter_03]\n",
    "\n",
    "def generate_gpt_user_prompt(robot_state_name, robot_state_description, param_list=param_list):\n",
    "    \n",
    "    random.shuffle(param_list) # Comment this out to remove randomness \n",
    "\n",
    "    return f'''\n",
    "{deployment_context}\n",
    "\n",
    "This {robot_morphology} robot is currently in state '{robot_state_name}' because {robot_state_description}\n",
    "\n",
    "{communication_style}\n",
    "\n",
    "Below is a list of three (3) acoustic parameters, complete with a description and a value range for each parameter. These parameters govern the characteristics of the robot's audio communication. The data in this list is in the format: [Parameter Name, Parameter Description, (Value Range)]\n",
    "\n",
    "{param_list[0]}\n",
    "\n",
    "{param_list[1]}\n",
    "\n",
    "{param_list[2]}\n",
    "\n",
    "Your Task:\n",
    "From the available three (3) acoustic parameters, please select at minimum one and a maximum three of the most relevant parameters to express the robot state '{robot_state_name}'. \n",
    "\n",
    "Please include reasonable values within the provided value ranges for each selected motion parameter. Please only select those that you believe are most relevant. \n",
    "\n",
    "Please include no additional text or explanation. There should be no blank lines\n",
    "\n",
    "Your response must keep the selected parameters in numerical order, and be in this exact format (with one line for every selected parameter):\n",
    "[##, Z], (Parameter Name, Value)\n",
    "[##, Z], (Parameter Name, Value) (optional)\n",
    "[##, Z], (Parameter Name, Value) (optional)\n",
    "\n",
    "## = parameter number (e.g. 02)\n",
    "Z = value option, either A, B, or C.\n",
    "Parameter Name = name of the selected parameter\n",
    "Value = value selected for that parameter\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f883cde",
   "metadata": {},
   "source": [
    "### Test Cell Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = all_states[0]\n",
    "current_description = all_states_descriptions[0]\n",
    "\n",
    "gpt_user_prompt = generate_gpt_user_prompt(current_state, current_description, param_list)\n",
    "\n",
    "print(f'Your prompt for GPT is: \\n\\n{gpt_assistant_prompt}\\n{gpt_user_prompt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1458a50-0a36-498e-bad4-b76e278ee8be",
   "metadata": {},
   "source": [
    "### Now build the OpenAI API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_quantity = 80\n",
    "gpt_model = \"gpt-4o\"                        # gpt-3.5-turbo     | Use this for dev/testing\n",
    "                                            # gpt-4 / gpt-4o    | Use this when deployed, more expensive\n",
    "                                            # gpt-4o-mini       | Lightweight, less expensive than gpt4o\n",
    "\n",
    "attempt_ID = '01'\n",
    "\n",
    "temperature_coefficient = 0.2           # Slight stochastic @ 0.2 to 0.5\n",
    "frequency_penalty_coefficient = 0.0     # Lightly penalize repetition @ 0.0 to 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf539e-9d4b-445c-b2b9-c581d622b8c0",
   "metadata": {},
   "source": [
    "### Multi-Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38a890-97e8-4ba4-bf5d-32be9fa5f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counter = 1\n",
    "\n",
    "for state_idx in range(0,len(all_states)):\n",
    "    sheet_name = all_states[state_idx] + '_' + attempt_ID \n",
    "    row_counter += 1 \n",
    "    selected_param_value_pairs = []\n",
    "\n",
    "    # Load Workbook and Fill Table Entrie\n",
    "    workbook_path = \"./../llm_audio_testcase/llm_audio_testcase.xlsx\"\n",
    "    response_book = load_workbook(workbook_path)\n",
    "\n",
    "    try: # Try to open existing sheet\n",
    "        response_sheet = response_book[sheet_name]\n",
    "    except KeyError:  # If ot doesn't exist. create it\n",
    "        response_sheet = response_book.create_sheet(title=sheet_name)\n",
    "\n",
    "    response_sheet[\"B1\"] = \"state\"\n",
    "    response_sheet[\"C1\"] = \"iteration\"\n",
    "    response_sheet[\"D1\"] = \"P1 BPM\"\n",
    "    response_sheet[\"E1\"] = \"P2 BPL\"\n",
    "    response_sheet[\"F1\"] = \"P3 PitchBend\"\n",
    "\n",
    "    # response_book.save(workbook_path)\n",
    "\n",
    "    print(f'\\n************************************')\n",
    "    print(f'********* ROBOT STATE : {all_states[state_idx]}')\n",
    "    print(f'************************************\\n')\n",
    "\n",
    "\n",
    "    for iteration in range(0, iteration_quantity):\n",
    "        print(f'~~~~~~~~~~~~ Iteration {iteration:02d}')\n",
    "\n",
    "        # adding to excel\n",
    "        response_sheet[\"B\"+str(iteration+2)] = all_states[state_idx]\n",
    "        response_sheet[\"C\"+str(iteration+2)] = iteration\n",
    "\n",
    "        # generate unique prompt for each iteration\n",
    "        gpt_prompt_pre_gen = generate_gpt_user_prompt(all_states[state_idx], all_states_descriptions[state_idx])\n",
    "        # print(f\"\\n\\n{gpt_prompt_pre_gen}\\n\\n\") <- # uncomment if you want to see prompt used at each step\n",
    "\n",
    "        # call GPT client\n",
    "        completion = client.chat.completions.create(\n",
    "            model=gpt_model,  # \"gpt-3.5-turbo\" (use this for dev/testing) or \"gpt-4\" (use this when deployed, more expensive)\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": gpt_assistant_prompt},\n",
    "                {\"role\": \"user\", \"content\": gpt_prompt_pre_gen}],\n",
    "            temperature=temperature_coefficient,         # 0.2 or 0.5\n",
    "            max_tokens=500,\n",
    "            frequency_penalty= frequency_penalty_coefficient  # 0.0 or 0.2\n",
    "            )\n",
    "        \n",
    "        # Printing result from call to GPT client\n",
    "        print(completion.choices[0].message.content, '\\n\\n')\n",
    "    \n",
    "        # Now iterate and count the responses\n",
    "        for line in completion.choices[0].message.content.split('\\n'):\n",
    "            print(f\"line: {line}\")\n",
    "\n",
    "            # Check if the line is blank or too short, skip if it is\n",
    "            if len(line) < 6:\n",
    "                continue  # Skip to the next iteration if line is blank or too short\n",
    "\n",
    "            try:\n",
    "                collected_identifier = line[1:3] + line[5]  # Assuming the line has the required format\n",
    "                print(f'Appending: {collected_identifier}')\n",
    "                selected_param_value_pairs.append(collected_identifier)\n",
    "\n",
    "                # Depending on the identifier, place the value in the right column\n",
    "                if collected_identifier[0:2] == '01':\n",
    "                    response_sheet[\"D\" + str(iteration + 2)] = collected_identifier[2]\n",
    "\n",
    "                elif collected_identifier[0:2] == '02':\n",
    "                    response_sheet[\"E\" + str(iteration + 2)] = collected_identifier[2]\n",
    "\n",
    "                elif collected_identifier[0:2] == '03':\n",
    "                    response_sheet[\"F\" + str(iteration + 2)] = collected_identifier[2]\n",
    "\n",
    "            except IndexError:\n",
    "                print(f\"Skipping line due to formatting issues: {line}\")\n",
    "                continue  # Skip the line if there's an IndexError (unexpected format)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'selected_param_value_pairs\\nLength: {len(selected_param_value_pairs)}\\n{selected_param_value_pairs}')\n",
    "\n",
    "    response_book.save(workbook_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dc83b-eb67-4e98-a895-bb2dea8e6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the collected data\n",
    "def pair_parser(pv_pairs, printer=None):\n",
    "\n",
    "    param01_valA = 0\n",
    "    param01_valB = 0\n",
    "    param01_valC = 0\n",
    "    param02_valA = 0\n",
    "    param02_valB = 0\n",
    "    param02_valC = 0\n",
    "    param03_valA = 0\n",
    "    param03_valB = 0\n",
    "    param03_valC = 0\n",
    "    \n",
    "    for pv_pair in pv_pairs:\n",
    "        if pv_pair[1] == '1':\n",
    "            if pv_pair[2] == 'A':\n",
    "                param01_valA +=1\n",
    "            if pv_pair[2] == 'B':\n",
    "                param01_valB +=1\n",
    "            if pv_pair[2] == 'C':\n",
    "                param01_valC +=1\n",
    "        if pv_pair[1] == '2':\n",
    "            if pv_pair[2] == 'A':\n",
    "                param02_valA +=1\n",
    "            if pv_pair[2] == 'B':\n",
    "                param02_valB +=1\n",
    "            if pv_pair[2] == 'C':\n",
    "                param02_valC +=1\n",
    "        if pv_pair[1] == '3':\n",
    "            if pv_pair[2] == 'A':\n",
    "                param03_valA +=1\n",
    "            if pv_pair[2] == 'B':\n",
    "                param03_valB +=1\n",
    "            if pv_pair[2] == 'C':\n",
    "                param03_valC +=1\n",
    "\n",
    "    # Sanity check\n",
    "    sum = param01_valA + param02_valA + param03_valA + param01_valB + param02_valB + param03_valB + param01_valC + param02_valC + param03_valC \n",
    "    original_length = len(pv_pairs)    \n",
    "    sanity_check = sum == original_length\n",
    "    \n",
    "    if printer:\n",
    "        print(f'sanity check: {sanity_check}\\n')\n",
    "        print('param01_valA =', param01_valA)\n",
    "        print('param01_valB =', param01_valB)\n",
    "        print('param01_valC =', param01_valC)\n",
    "        print('param02_valA =', param02_valA)\n",
    "        print('param02_valB =', param02_valB)\n",
    "        print('param02_valC =', param02_valC)\n",
    "        print('param03_valA =', param03_valA)\n",
    "        print('param03_valB =', param03_valB)\n",
    "        print('param03_valC =', param03_valC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a58cd9-2194-4e49-a69d-b71db08b6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the last state, lets just see if the outputs are actually giving us data in the correct format:\n",
    "pair_parser(pv_pairs=selected_param_value_pairs, printer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b828d-3bce-4b66-ab77-b58a62f3494c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Single Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd693b-54eb-449e-a791-e229094345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # \"gpt-3.5-turbo\" (use this for dev/testing) or \"gpt-4\" (use this when deployed, more expensive)\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": gpt_assistant_prompt},\n",
    "        {\"role\": \"user\", \"content\": gpt_user_prompt}],\n",
    "    temperature=0.5,\n",
    "    max_tokens=500,\n",
    "    frequency_penalty=0\n",
    ")\n",
    "\n",
    "print('Raw Model Output:\\n\\n')\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d0977-a7e6-40a5-ac10-1182b64138eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Robot State: {robot_state_name}\\n')\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1e13d-d6b2-44ce-98f4-fa9651721a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
